import os
import json
import argparse
import asyncio
import logging
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
from io import BytesIO
from collections import deque
import httpx
from httpx import HTTPStatusError, ReadTimeout, Timeout
from PIL import Image

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π –≤ posted.json ---
MAX_POSTED_RECORDS = 100

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≤–æ—Ç–µ—Ä–º–∞—Ä–∫–∏ ---
WATERMARK_SCALE = 0.35  # 35% –æ—Ç —à–∏—Ä–∏–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
HTTPX_TIMEOUT = Timeout(connect=10.0, read=60.0, write=10.0, pool=5.0)
MAX_RETRIES   = 3
RETRY_DELAY   = 5.0
DEFAULT_DELAY = 10.0

def escape_html(text: str) -> str:
    """
    –≠–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã HTML (<, >, &, ") –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.
    """
    text = text.replace("&", "&amp;")
    text = text.replace("<", "&lt;")
    text = text.replace(">", "&gt;")
    text = text.replace('"', "&quot;")
    return text

def chunk_text(text: str, size: int = 4096) -> List[str]:
    """
    –î–µ–ª–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª–∏–Ω–æ–π <= size, —Å–æ—Ö—Ä–∞–Ω—è—è –∞–±–∑–∞—Ü—ã.
    """
    norm = text.replace('\r\n', '\n')
    paras = [p for p in norm.split('\n\n') if p.strip()]
    chunks, curr = [], ""

    def split_long(p: str) -> List[str]:
        parts, sub = [], ""
        for w in p.split(" "):
            if len(sub) + len(w) + 1 > size:
                parts.append(sub)
                sub = w
            else:
                sub = (sub + " " + w).lstrip()
        if sub:
            parts.append(sub)
        return parts

    for p in paras:
        if len(p) > size:
            if curr:
                chunks.append(curr)
                curr = ""
            chunks.extend(split_long(p))
        else:
            if not curr:
                curr = p
            elif len(curr) + 2 + len(p) <= size:
                curr += "\n\n" + p
            else:
                chunks.append(curr)
                curr = p

    if curr:
        chunks.append(curr)
    return chunks


def apply_watermark(img_path: Path, scale: float = 0.35) -> bytes:
    """
    –ù–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç watermark.png –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –≤–∏–¥–µ –±–∞–π—Ç–æ–≤ JPEG.
    """
    try:
        base_img = Image.open(img_path).convert("RGBA")
        base_width, _ = base_img.size

        watermark_path = Path(__file__).parent / "watermark.png"
        if not watermark_path.exists():
            logging.warning("–§–∞–π–ª watermark.png –Ω–µ –Ω–∞–π–¥–µ–Ω. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")
            img_byte_arr = BytesIO()
            base_img.convert("RGB").save(img_byte_arr, format='JPEG', quality=90)
            return img_byte_arr.getvalue()

        watermark_img = Image.open(watermark_path).convert("RGBA")
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–¥—è–Ω–æ–≥–æ –∑–Ω–∞–∫–∞
        wm_width, wm_height = watermark_img.size
        new_wm_width = int(base_width * scale)
        new_wm_height = int(wm_height * (new_wm_width / wm_width))
        resample_filter = getattr(Image.Resampling, "LANCZOS", Image.LANCZOS)
        watermark_img = watermark_img.resize((new_wm_width, new_wm_height), resample=resample_filter)
        
        # –°–æ–∑–¥–∞—ë–º –ø—É—Å—Ç–æ–π —Å–ª–æ–π –†–ê–ó–ú–ï–†–û–ú –° –û–°–ù–û–í–ù–û–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï
        overlay = Image.new("RGBA", base_img.size, (0, 0, 0, 0))

        # –ù–∞–Ω–æ—Å–∏–º –≤–æ—Ç–µ—Ä–º–∞—Ä–∫—É –Ω–∞ —ç—Ç–æ—Ç —Å–ª–æ–π –≤ –Ω—É–∂–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
        padding = int(base_width * 0.02)
        position = (base_width - new_wm_width - padding, padding)
        overlay.paste(watermark_img, position, watermark_img) # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞—Å–∫—É –¥–ª—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏

        # –°–æ–≤–º–µ—â–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ –°–õ–û–ï–ú (overlay), –∞ –Ω–µ —Å –º–∞–ª–µ–Ω—å–∫–æ–π –≤–æ—Ç–µ—Ä–º–∞—Ä–∫–æ–π
        composite_img = Image.alpha_composite(base_img, overlay).convert("RGB")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–π—Ç—ã –∫–∞–∫ JPEG –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
        img_byte_arr = BytesIO()
        composite_img.save(img_byte_arr, format='JPEG', quality=90)
        return img_byte_arr.getvalue()
        
    except Exception as e:
        logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–ª–æ–∂–∏—Ç—å –≤–æ–¥—è–Ω–æ–π –∑–Ω–∞–∫ –Ω–∞ {img_path}: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        try:
            with open(img_path, 'rb') as f:
                return f.read()
        except Exception as e_orig:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–∞–∂–µ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_path}: {e_orig}")
            return b""
async def _post_with_retry(
    client: httpx.AsyncClient,
    method: str,
    url: str,
    data: Dict[str, Any],
    files: Optional[Dict[str, Any]] = None
) -> bool:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP POST-–∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏.
    """
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = await client.request(method, url, data=data, files=files, timeout=HTTPX_TIMEOUT)
            resp.raise_for_status()
            return True
        except ReadTimeout:
            logging.warning("‚è± Timeout %s/%s for %s", attempt, MAX_RETRIES, url)
        except HTTPStatusError as e:
            code = e.response.status_code
            text = e.response.text
            if code == 429:
                info = e.response.json().get("parameters", {})
                wait = info.get("retry_after", RETRY_DELAY)
                logging.warning("üê¢ Rate limited %s/%s: retry after %s seconds", attempt, MAX_RETRIES, wait)
                await asyncio.sleep(wait)
                continue
            if 400 <= code < 500:
                logging.error("‚ùå %s %s: %s", method, code, text)
                return False
            logging.warning("‚ö†Ô∏è %s %s, retry %s/%s", method, code, attempt, MAX_RETRIES)
        except httpx.RequestError as e:
            logging.warning(f"Request error on attempt {attempt + 1}/{MAX_RETRIES}: {e}")
        except Exception as e:
            logging.error(f"An unexpected error occurred on attempt {attempt + 1}/{MAX_RETRIES}: {e}")

        await asyncio.sleep(RETRY_DELAY)

    logging.error("‚ò†Ô∏è Failed %s after %s attempts", url, MAX_RETRIES)
    return False


async def send_media_group(
    client: httpx.AsyncClient,
    token: str,
    chat_id: str,
    images: List[Path]
) -> bool:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∞–ª—å–±–æ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –±–µ–∑ –ø–æ–¥–ø–∏—Å–∏.
    """
    url = f"https://api.telegram.org/bot{token}/sendMediaGroup"
    media = []
    files = {}

    if not images:
        logging.warning("No images provided for media group.")
        return False

    for idx, img_path in enumerate(images[:10]): # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Telegram –Ω–∞ 10 —Ñ–æ—Ç–æ
        try:
            image_bytes = apply_watermark(img_path, scale=WATERMARK_SCALE)
            if not image_bytes:
                logging.warning(f"Skipping image {img_path} due to empty bytes.")
                continue

            key = f"file{idx}"
            files[key] = (img_path.name, image_bytes, "image/jpeg")
            media.append({"type": "photo", "media": f"attach://{key}"})
        except Exception as e:
            logging.error(f"Error processing image {img_path} for media group: {e}")

    if not media:
        logging.warning("No valid images to send in media group.")
        return False

    data = {
        "chat_id": chat_id,
        "media": json.dumps(media, ensure_ascii=False)
    }
    return await _post_with_retry(client, "POST", url, data, files)


async def send_message(
    client: httpx.AsyncClient,
    token: str,
    chat_id: str,
    text: str,
    reply_markup: Optional[Dict[str, Any]] = None
) -> bool:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —Ä–∞–∑–±–æ—Ä–æ–º HTML.
    """
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    data = {
        "chat_id": chat_id,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }
    if reply_markup:
        data["reply_markup"] = json.dumps(reply_markup, ensure_ascii=False)
    return await _post_with_retry(client, "POST", url, data)


def validate_article(
    art: Dict[str, Any],
    article_dir: Path
) -> Optional[Tuple[str, Path, List[Path], str]]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–∫–∏ —Å—Ç–∞—Ç—å–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
    """
    aid = art.get("id")
    title = art.get("title", "").strip()
    txt_name = Path(art.get("text_file", "")).name if art.get("text_file") else None

    if not all([aid, title, txt_name]):
        logging.error("Invalid meta.json in %s (missing id, title, or text_file).", article_dir)
        return None

    text_path = article_dir / txt_name
    if not text_path.is_file():
        logging.error("Text file %s not found for article in %s. Skipping.", text_path, article_dir)
        return None

    valid_imgs = sorted([
        article_dir / "images" / Path(p).name for p in art.get("images", [])
        if (article_dir / "images" / Path(p).name).is_file()
    ])

    html_title = f"<b>{escape_html(title)}</b>"
    return html_title, text_path, valid_imgs, title


def load_posted_ids(state_file: Path) -> Set[str]:
    """
    –ß–∏—Ç–∞–µ—Ç state-—Ñ–∞–π–ª, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç set –∏–∑ ID –≤ –≤–∏–¥–µ –°–¢–†–û–ö.
    –û–±—Ä–µ–∑–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ MAX_POSTED_RECORDS, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–∞–º—ã–µ –Ω–æ–≤—ã–µ.
    """
    if not state_file.is_file():
        return set()
    
    ids_from_file: List[str] = []
    try:
        data = json.loads(state_file.read_text(encoding="utf-8"))
        if isinstance(data, list):
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            ids_from_file = [str(item) for item in data if item is not None]
    except (json.JSONDecodeError, Exception) as e:
        logging.warning(f"Error reading or parsing state file {state_file}: {e}. Returning empty set.")

    # –û–±—Ä–µ–∑–∞–µ–º —Å–ø–∏—Å–æ–∫, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–∞–º—ã–µ –Ω–æ–≤—ã–µ (–≤ –∫–æ–Ω—Ü–µ —Å–ø–∏—Å–∫–∞)
    if len(ids_from_file) > MAX_POSTED_RECORDS:
        start_index = len(ids_from_file) - MAX_POSTED_RECORDS
        ids_from_file = ids_from_file[start_index:]
        logging.info(f"State file trimmed to last {MAX_POSTED_RECORDS} records during load.")

    return set(ids_from_file)


def save_posted_ids(all_ids_to_save: Set[str], state_file: Path) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ ID (–∫–∞–∫ —Å—Ç—Ä–æ–∫–∏) –≤ —Ñ–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è.
    """
    state_file.parent.mkdir(parents=True, exist_ok=True)
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫ '10', '2' -> ['10', '2'].
        # –î–ª—è —á–∏—Å–ª–æ–≤–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏: sorted(list(all_ids_to_save), key=int)
        final_list_to_save = sorted(list(all_ids_to_save), key=int)

        with state_file.open("w", encoding="utf-8") as f:
            json.dump(final_list_to_save, f, ensure_ascii=False, indent=2)
        logging.info(f"Saved {len(final_list_to_save)} IDs to state file {state_file}")
    except Exception as e:
        logging.error(f"Failed to save state file {state_file}: {e}")


async def main(parsed_dir: str, state_path: str, limit: Optional[int]):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ—Å—Ç–µ—Ä–∞.
    """
    token = os.getenv("TELEGRAM_TOKEN")
    chat_id = os.getenv("TELEGRAM_CHANNEL")
    if not token or not chat_id:
        logging.error("TELEGRAM_TOKEN and TELEGRAM_CHANNEL environment variables must be set.")
        return

    delay = float(os.getenv("POST_DELAY", DEFAULT_DELAY))
    parsed_root = Path(parsed_dir)
    state_file = Path(state_path)

    if not parsed_root.is_dir():
        logging.error(f"Directory {parsed_root} does not exist. Exiting.")
        return

    posted_ids = load_posted_ids(state_file)
    logging.info(f"Loaded {len(posted_ids)} previously posted IDs from {state_file.name}.")

    articles_to_post: List[Dict[str, Any]] = []
    for d in sorted(parsed_root.iterdir()):
        meta_file = d / "meta.json"
        if d.is_dir() and meta_file.is_file():
            try:
                art_meta = json.loads(meta_file.read_text(encoding="utf-8"))
                # –†–∞–±–æ—Ç–∞–µ–º —Å ID –∫–∞–∫ —Å–æ —Å—Ç—Ä–æ–∫–∞–º–∏
                article_id = str(art_meta.get("id"))
                if article_id and article_id != 'None' and article_id not in posted_ids:
                    if validated_data := validate_article(art_meta, d):
                        html_title, text_path, image_paths, original_title = validated_data
                        articles_to_post.append({
                            "id": article_id,
                            "html_title": html_title,
                            "text_path": text_path,
                            "image_paths": image_paths,
                            "original_title": original_title
                        })
            except (json.JSONDecodeError, Exception) as e:
                logging.warning(f"Could not process meta.json in {d.name}: {e}. Skipping.")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ ID (—á–∏—Å–ª–æ–≤–æ–º—É)
    articles_to_post.sort(key=lambda x: int(x["id"]))

    if not articles_to_post:
        logging.info("üîç No new articles to post. Exiting.")
        return

    logging.info(f"Found {len(articles_to_post)} new articles to consider for posting.")

    client = httpx.AsyncClient()
    sent_count = 0
    newly_posted_ids: Set[str] = set()

    for article in articles_to_post:
        if limit is not None and sent_count >= limit:
            logging.info(f"Posting limit of {limit} reached. Stopping.")
            break

        aid = article["id"]
        logging.info(f"Attempting to post article ID={aid}")
        
        posted_successfully = False
        try:
            # –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –º–µ–¥–∏–∞ –≥—Ä—É–ø–ø—É, –µ—Å–ª–∏ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if article["image_paths"]:
                if not await send_media_group(client, token, chat_id, article["image_paths"]):
                    logging.warning(f"Failed to send media group for ID={aid}. Continuing to send text.")

            raw_text = article["text_path"].read_text(encoding="utf-8")
            # –£–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –Ω–∞—á–∞–ª–∞ —Ç–µ–∫—Å—Ç–∞, –µ—Å–ª–∏ –æ–Ω —Ç–∞–º –µ—Å—Ç—å
            cleaned_text = raw_text.lstrip()
            if cleaned_text.startswith(article["original_title"]):
                 cleaned_text = cleaned_text[len(article["original_title"]):].lstrip()

            full_html_content = f"{article['html_title']}\n\n{escape_html(cleaned_text)}"
            chunks = chunk_text(full_html_content)

            all_chunks_sent = True
            for i, part in enumerate(chunks):
                reply_markup = None
                if i == len(chunks) - 1: # –î–æ–±–∞–≤–ª—è–µ–º –∫–Ω–æ–ø–∫–∏ –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Å–æ–æ–±—â–µ–Ω–∏—é
                    reply_markup = {
                        "inline_keyboard": [[
                            {"text": "–û–±–º–µ–Ω –≤–∞–ª—é—Ç", "url": "https://t.me/mister1dollar"},
                            {"text": "–û—Ç–∑—ã–≤—ã", "url": "https://t.me/feedback1dollar"}
                        ]]
                    }
                
                if not await send_message(client, token, chat_id, part, reply_markup=reply_markup):
                    logging.error(f"Failed to send text chunk for ID={aid}. Skipping article.")
                    all_chunks_sent = False
                    break
            
            if all_chunks_sent:
                posted_successfully = True

        except Exception as e:
            logging.error(f"‚ùå An error occurred while posting article ID={aid}: {e}", exc_info=True)

        if posted_successfully:
            newly_posted_ids.add(aid)
            sent_count += 1
            logging.info(f"‚úÖ Successfully posted ID={aid}")
        
        await asyncio.sleep(delay)

    await client.aclose()

    if newly_posted_ids:
        all_ids_to_save = posted_ids.union(newly_posted_ids)
        save_posted_ids(all_ids_to_save, state_file)
    
    logging.info(f"üì¢ Finished: Sent {sent_count} articles in this run.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Poster: posts articles in batches to Telegram"
    )
    parser.add_argument(
        "--parsed-dir",
        type=str,
        default="articles",
        help="directory with parsed articles"
    )
    parser.add_argument(
        "--state-file",
        type=str,
        default="articles/posted.json",
        help="path to the state file"
    )
    parser.add_argument(
        "-n", "--limit",
        type=int,
        default=None,
        help="maximum number of articles to send"
    )
    args = parser.parse_args()
    asyncio.run(main(
        parsed_dir=args.parsed_dir,
        state_path=args.state_file,
        limit=args.limit
    ))

