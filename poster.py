import os
import json
import argparse
import asyncio
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
from io import BytesIO
import httpx
from httpx import HTTPStatusError, ReadTimeout, Timeout
from PIL import Image

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π –≤ posted.json ---
MAX_POSTED_RECORDS = 100

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≤–æ—Ç–µ—Ä–º–∞—Ä–∫–∏ ---
WATERMARK_SCALE = 0.35  # 35% –æ—Ç —à–∏—Ä–∏–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
HTTPX_TIMEOUT = Timeout(connect=10.0, read=60.0, write=10.0, pool=5.0)
MAX_RETRIES   = 3
RETRY_DELAY   = 5.0
DEFAULT_DELAY = 10.0

def escape_html(text: str) -> str:
    """
    –≠–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã HTML (<, >, &, ") –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.
    """
    return text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace('"', "&quot;")

def chunk_text(text: str, size: int = 4096) -> List[str]:
    """
    –î–µ–ª–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª–∏–Ω–æ–π <= size, —Å–æ—Ö—Ä–∞–Ω—è—è –∞–±–∑–∞—Ü—ã.
    """
    paras = [p for p in text.replace('\r\n', '\n').split('\n\n') if p.strip()]
    chunks, current_chunk = [], ""

    for p in paras:
        if len(p) > size:
            if current_chunk:
                chunks.append(current_chunk)
            # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –≥—Ä—É–±–æ –¥–µ–ª–∏–º –µ–≥–æ –ø–æ —Å–ª–æ–≤–∞–º
            parts = []
            sub_part = ""
            for word in p.split():
                if len(sub_part) + len(word) + 1 > size:
                    parts.append(sub_part)
                    sub_part = word
                else:
                    sub_part = f"{sub_part} {word}".lstrip()
            if sub_part:
                parts.append(sub_part)
            chunks.extend(parts)
            current_chunk = ""
        else:
            if not current_chunk:
                current_chunk = p
            elif len(current_chunk) + len(p) + 2 <= size:
                current_chunk += f"\n\n{p}"
            else:
                chunks.append(current_chunk)
                current_chunk = p
    
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

def apply_watermark(img_path: Path, scale: float = 0.35) -> bytes:
    """
    –ù–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç watermark.png –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –≤–∏–¥–µ –±–∞–π—Ç–æ–≤ JPEG.
    """
    try:
        base_img = Image.open(img_path).convert("RGBA")
        base_width, _ = base_img.size

        watermark_path = Path(__file__).parent / "watermark.png"
        if not watermark_path.exists():
            logging.warning("–§–∞–π–ª watermark.png –Ω–µ –Ω–∞–π–¥–µ–Ω. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")
            img_byte_arr = BytesIO()
            base_img.convert("RGB").save(img_byte_arr, format='JPEG', quality=90)
            return img_byte_arr.getvalue()

        watermark_img = Image.open(watermark_path).convert("RGBA")
        
        wm_width, wm_height = watermark_img.size
        new_wm_width = int(base_width * scale)
        new_wm_height = int(wm_height * (new_wm_width / wm_width))
        resample_filter = getattr(Image.Resampling, "LANCZOS", Image.LANCZOS)
        watermark_img = watermark_img.resize((new_wm_width, new_wm_height), resample=resample_filter)
        
        overlay = Image.new("RGBA", base_img.size, (0, 0, 0, 0))

        padding = int(base_width * 0.02)
        position = (base_width - new_wm_width - padding, padding)
        overlay.paste(watermark_img, position, watermark_img)

        composite_img = Image.alpha_composite(base_img, overlay).convert("RGB")

        img_byte_arr = BytesIO()
        composite_img.save(img_byte_arr, format='JPEG', quality=90)
        return img_byte_arr.getvalue()
        
    except Exception as e:
        logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–ª–æ–∂–∏—Ç—å –≤–æ–¥—è–Ω–æ–π –∑–Ω–∞–∫ –Ω–∞ {img_path}: {e}")
        try:
            with open(img_path, 'rb') as f:
                return f.read()
        except Exception as e_orig:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–∞–∂–µ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_path}: {e_orig}")
            return b""

async def _post_with_retry(
    client: httpx.AsyncClient,
    method: str,
    url: str,
    data: Dict[str, Any],
    files: Optional[Dict[str, Any]] = None
) -> bool:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP POST-–∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = await client.request(method, url, data=data, files=files, timeout=HTTPX_TIMEOUT)
            resp.raise_for_status()
            return True
        except HTTPStatusError as e:
            if e.response.status_code == 429:
                retry_after = int(e.response.json().get("parameters", {}).get("retry_after", RETRY_DELAY))
                logging.warning(f"üê¢ Rate limited. Retrying after {retry_after} seconds...")
                await asyncio.sleep(retry_after)
            elif 400 <= e.response.status_code < 500:
                logging.error(f"‚ùå Client error {e.response.status_code}: {e.response.text}")
                return False
            else:
                logging.warning(f"‚ö†Ô∏è Server error {e.response.status_code}. Retry {attempt}/{MAX_RETRIES}...")
                await asyncio.sleep(RETRY_DELAY * attempt)
        except (ReadTimeout, httpx.RequestError) as e:
            logging.warning(f"‚è±Ô∏è Network error: {e}. Retry {attempt}/{MAX_RETRIES}...")
            await asyncio.sleep(RETRY_DELAY * attempt)
    logging.error(f"‚ò†Ô∏è Failed to send request to {url} after {MAX_RETRIES} attempts.")
    return False

async def send_media_group(client: httpx.AsyncClient, token: str, chat_id: str, images: List[Path]) -> bool:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∞–ª—å–±–æ–º —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π."""
    url = f"https://api.telegram.org/bot{token}/sendMediaGroup"
    media, files = [], {}
    for idx, img_path in enumerate(images[:10]):
        image_bytes = apply_watermark(img_path)
        if image_bytes:
            key = f"photo{idx}"
            files[key] = (img_path.name, image_bytes, "image/jpeg")
            media.append({"type": "photo", "media": f"attach://{key}"})
    if not media: return False
    data = {"chat_id": chat_id, "media": json.dumps(media)}
    return await _post_with_retry(client, "POST", url, data, files)

async def send_message(client: httpx.AsyncClient, token: str, chat_id: str, text: str, **kwargs) -> bool:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ."""
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    data = {"chat_id": chat_id, "text": text, "parse_mode": "HTML", "disable_web_page_preview": True}
    if kwargs.get("reply_markup"):
        data["reply_markup"] = json.dumps(kwargs["reply_markup"])
    return await _post_with_retry(client, "POST", url, data)

def validate_article(
    art: Dict[str, Any],
    article_dir: Path
) -> Optional[Tuple[str, Path, List[Path], str]]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–∫–∏ —Å—Ç–∞—Ç—å–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
    """
    aid = art.get("id")
    title = art.get("title", "").strip()
    
    # –°—Ç—Ä–æ–≥–æ —Ç—Ä–µ–±—É–µ–º, —á—Ç–æ–±—ã –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –±—ã–ª –≤ meta.json
    text_file_path_str = art.get("text_file")

    if not all([aid, title, text_file_path_str]):
        logging.error(f"Invalid meta.json in {article_dir} (missing id, title, or text_file).")
        return None

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ—Ç —Ñ–∞–π–ª —Ä–µ–∞–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    text_path = Path(text_file_path_str)
    if not text_path.is_file():
        logging.error(f"Text file {text_path} specified in meta.json not found. Skipping.")
        return None

    # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –≤ –ø–æ–¥–ø–∞–ø–∫–µ /images –¥–ª—è –ø–æ—Ä—è–¥–∫–∞
    images_dir = article_dir / "images"
    valid_imgs: List[Path] = []
    if images_dir.is_dir():
        valid_imgs = sorted([p for p in images_dir.iterdir() if p.suffix.lower() in {".jpg", ".jpeg", ".png"}])

    html_title = f"<b>{escape_html(title)}</b>"
    
    return html_title, text_path, valid_imgs, title

def load_posted_ids(state_file: Path) -> Set[str]:
    """–ß–∏—Ç–∞–µ—Ç state-—Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç set –∏–∑ ID –≤ –≤–∏–¥–µ –°–¢–†–û–ö."""
    if not state_file.is_file(): return set()
    try:
        data = json.loads(state_file.read_text(encoding="utf-8"))
        ids = {str(item) for item in data if item is not None} if isinstance(data, list) else set()
        return set(list(ids)[-MAX_POSTED_RECORDS:])
    except (json.JSONDecodeError, Exception) as e:
        logging.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è {state_file}: {e}.")
        return set()

def save_posted_ids(all_ids_to_save: Set[str], state_file: Path) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ ID –≤ —Ñ–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è."""
    state_file.parent.mkdir(parents=True, exist_ok=True)
    try:
        sorted_ids = sorted(list(all_ids_to_save), key=int)
        with state_file.open("w", encoding="utf-8") as f:
            json.dump(sorted_ids, f, ensure_ascii=False, indent=2)
        logging.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(sorted_ids)} ID –≤ —Ñ–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è {state_file}.")
    except Exception as e:
        logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è {state_file}: {e}")

async def main(parsed_dir: str, state_path: str, limit: Optional[int]):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ—Å—Ç–µ—Ä–∞."""
    token, chat_id = os.getenv("TELEGRAM_TOKEN"), os.getenv("TELEGRAM_CHANNEL")
    if not token or not chat_id:
        logging.error("–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è TELEGRAM_TOKEN –∏ TELEGRAM_CHANNEL –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã.")
        return

    parsed_root, state_file = Path(parsed_dir), Path(state_path)
    if not parsed_root.is_dir():
        logging.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {parsed_root} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –í—ã—Ö–æ–¥.")
        return

    posted_ids = load_posted_ids(state_file)
    logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(posted_ids)} —Ä–∞–Ω–µ–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö ID.")

    articles_to_post = []
    for d in sorted(parsed_root.iterdir()):
        meta_file = d / "meta.json"
        if d.is_dir() and meta_file.is_file():
            try:
                art_meta = json.loads(meta_file.read_text(encoding="utf-8"))
                article_id = str(art_meta.get("id"))
                if article_id and article_id != 'None' and article_id not in posted_ids:
                    if validated_data := validate_article(art_meta, d):
                        _, text_path, image_paths, original_title = validated_data
                        articles_to_post.append({
                            "id": article_id,
                            "html_title": f"<b>{escape_html(original_title)}</b>",
                            "text_path": text_path,
                            "image_paths": image_paths,
                            "original_title": original_title
                        })
            except Exception as e:
                logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å {d.name}: {e}")

    articles_to_post.sort(key=lambda x: int(x["id"]))
    if not articles_to_post:
        logging.info("üîç –ù–µ—Ç –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏.")
        return

    logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(articles_to_post)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏.")
    
    async with httpx.AsyncClient() as client:
        sent_count = 0
        newly_posted_ids: Set[str] = set()
        
        for article in articles_to_post:
            if limit is not None and sent_count >= limit:
                logging.info(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {limit} —Å—Ç–∞—Ç–µ–π.")
                break
            
            logging.info(f"–ü—É–±–ª–∏–∫—É–µ–º —Å—Ç–∞—Ç—å—é ID={article['id']}...")
            try:
                if article["image_paths"]:
                    await send_media_group(client, token, chat_id, article["image_paths"])

                raw_text = article["text_path"].read_text(encoding="utf-8")
                cleaned_text = raw_text.lstrip()
                if cleaned_text.startswith(article["original_title"]):
                    cleaned_text = cleaned_text[len(article["original_title"]):].lstrip()

                full_html = f"{article['html_title']}\n\n{escape_html(cleaned_text)}"
                chunks = chunk_text(full_html)

                for i, chunk in enumerate(chunks):
                    is_last_chunk = (i == len(chunks) - 1)
                    reply_markup = {
                        "inline_keyboard": [[
                            {"text": "–û–±–º–µ–Ω –≤–∞–ª—é—Ç", "url": "https://t.me/mister1dollar"},
                            {"text": "–û—Ç–∑—ã–≤—ã", "url": "https://t.me/feedback1dollar"}
                        ]]
                    } if is_last_chunk else None
                    if not await send_message(client, token, chat_id, chunk, reply_markup=reply_markup):
                        raise Exception("Failed to send a message chunk.")

                logging.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ ID={article['id']}")
                newly_posted_ids.add(article['id'])
                sent_count += 1

            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ ID={article['id']}: {e}", exc_info=True)
            
            await asyncio.sleep(float(os.getenv("POST_DELAY", DEFAULT_DELAY)))

    if newly_posted_ids:
        all_ids_to_save = posted_ids.union(newly_posted_ids)
        save_posted_ids(all_ids_to_save, state_file)
    
    logging.info(f"üì¢ –ó–∞–≤–µ—Ä—à–µ–Ω–æ: –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {sent_count} —Å—Ç–∞—Ç–µ–π.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ü—É–±–ª–∏–∫—É–µ—Ç —Å—Ç–∞—Ç—å–∏ –≤ Telegram.")
    parser.add_argument("--parsed-dir", type=str, default="articles", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ —Å—Ç–∞—Ç—å—è–º–∏.")
    parser.add_argument("--state-file", type=str, default="articles/posted.json", help="–§–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è.")
    parser.add_argument("-n", "--limit", type=int, default=None, help="–õ–∏–º–∏—Ç —Å—Ç–∞—Ç–µ–π –∑–∞ –∑–∞–ø—É—Å–∫.")
    args = parser.parse_args()
    asyncio.run(main(args.parsed_dir, args.state_file, args.limit))
