import argparse
import logging
import json
import hashlib
import time
import re
import os
import shutil
import html
import fcntl
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Set

import requests 
from bs4 import BeautifulSoup
from curl_cffi import requests as cffi_requests, CurlHttpVersion

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
OUTPUT_DIR = Path("articles")
CATALOG_PATH = OUTPUT_DIR / "catalog.json"
MAX_RETRIES = 3
BASE_DELAY = 1.0
MAX_POSTED_RECORDS = 500 # –£–≤–µ–ª–∏—á–∏–ª–∏ –ø–∞–º—è—Ç—å
FETCH_DEPTH = 500

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è –ø–æ—Ä—Ç–∞ WARP (Socks5 —Å —É–¥–∞–ª–µ–Ω–Ω—ã–º DNS)
WARP_PROXY = "socks5h://127.0.0.1:40000"

# --- –ù–ê–°–¢–†–û–ô–ö–ò –°–ï–¢–ò ---
SCRAPER = cffi_requests.Session(
    impersonate="chrome110",
    proxies={
        "http": WARP_PROXY,
        "https": WARP_PROXY
    },
    http_version=CurlHttpVersion.V1_1
)

IPHONE_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://www.google.com/",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "cross-site",
    "Upgrade-Insecure-Requests": "1"
}

SCRAPER.headers = IPHONE_HEADERS
SCRAPER_TIMEOUT = 60 
BAD_RE = re.compile(r"[\u200b-\u200f\uFEFF\u200E\u00A0]")

# --- –ü–†–Ø–ú–û–ô –ü–ï–†–ï–í–û–î–ß–ò–ö ---
def translate_text(text: str, to_lang: str = "ru") -> str:
    if not text: return ""

    chunks = []
    current_chunk = ""
    for paragraph in text.split('\n'):
        if len(paragraph) > 1800:
            if current_chunk:
                chunks.append(current_chunk)
                current_chunk = ""
            chunks.append(paragraph)
            continue

        if len(current_chunk) + len(paragraph) < 1800:
            current_chunk += paragraph + "\n"
        else:
            chunks.append(current_chunk)
            current_chunk = paragraph + "\n"
    if current_chunk: chunks.append(current_chunk)

    translated_parts = []
    url = "https://translate.googleapis.com/translate_a/single"
    # –î–ª—è –≥—É–≥–ª–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π requests –±–µ–∑ –ø—Ä–æ–∫—Å–∏, —Ç–∞–∫ –Ω–∞–¥–µ–∂–Ω–µ–µ
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"}

    for chunk in chunks:
        if not chunk.strip():
            translated_parts.append("")
            continue
        try:
            params = {
                "client": "gtx", "sl": "en", "tl": to_lang, "dt": "t", "q": chunk.strip()
            }
            r = requests.get(url, params=params, headers=headers, timeout=10)
            if r.status_code == 200:
                data = r.json()
                text_part = "".join([item[0] for item in data[0] if item and item[0]])
                translated_parts.append(text_part)
            else:
                translated_parts.append(chunk)
            time.sleep(0.3)
        except Exception:
            translated_parts.append(chunk)

    return "\n".join(translated_parts)

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï ---
def sanitize_text(text: str) -> str:
    if not text: return ""
    text = html.unescape(text)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'mce_SELRES_[^ ]+', '', text)
    return re.sub(r'\n{3,}', '\n\n', text).strip()

def load_posted_ids(state_file_path: Path) -> Set[str]:
    try:
        if state_file_path.exists():
            with open(state_file_path, 'r', encoding='utf-8') as f:
                fcntl.flock(f, fcntl.LOCK_SH)
                data = json.load(f)
                return {str(item) for item in data}
        return set()
    except Exception: return set()

def load_stopwords(file_path: Optional[Path]) -> List[str]:
    if not file_path or not file_path.exists(): return []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return [line.strip().lower() for line in f if line.strip()]
    except Exception: return []

# --- –ö–ê–†–¢–ò–ù–ö–ò ---
def extract_img_url(img_tag: Any) -> Optional[str]:
    width_attr = img_tag.get("width")
    if width_attr and width_attr.isdigit():
        if int(width_attr) < 400: return None

    def is_junk(url_str: str) -> bool:
        u = url_str.lower()
        bad = ["gif", "logo", "banner", "icon", "avatar", "button", "share", "pixel", "tracker"]
        if any(b in u for b in bad): return True
        if re.search(r'-\d{2,3}x\d{2,3}\.', u): return True
        return False

    srcset = img_tag.get("srcset") or img_tag.get("data-srcset")
    if srcset:
        try:
            parts = srcset.split(',')
            links = []
            for p in parts:
                match = re.search(r'(\S+)\s+(\d+)w', p.strip())
                if match: 
                    w_val = int(match.group(2))
                    u_val = match.group(1)
                    if w_val >= 400: links.append((w_val, u_val))
            if links:
                best_link = sorted(links, key=lambda x: x[0], reverse=True)[0][1]
                if not is_junk(best_link): 
                    return best_link.split('?')[0]
        except Exception: pass

    attrs = ["data-orig-file", "data-large-file", "data-src", "data-lazy-src", "src"]
    for attr in attrs:
        if val := img_tag.get(attr):
            clean_val = val.split()[0].split(',')[0].split('?')[0]
            if not is_junk(clean_val): return clean_val

    return None

def save_image(url, folder):
    folder.mkdir(parents=True, exist_ok=True)
    fn = url.rsplit('/',1)[-1].split('?',1)[0]
    if len(fn) > 50: fn = hashlib.md5(fn.encode()).hexdigest() + ".jpg"
    dest = folder / fn
    try:
        resp = make_request("GET", url) 
        dest.write_bytes(resp.content)
        return str(dest)
    except Exception: return None

# --- –ó–ê–ü–†–û–°–´ (–° –ü–û–í–¢–û–†–ê–ú–ò –ò WARP) ---
def make_request(method: str, url: str, **kwargs):
    retries = 3
    for i in range(retries):
        try:
            kwargs.setdefault("timeout", SCRAPER_TIMEOUT)
            
            if method.upper() == "GET":
                response = SCRAPER.get(url, **kwargs)
            else:
                response = SCRAPER.request(method, url, **kwargs)

            if response.status_code in [403, 429]:
                logging.warning(f"‚ö†Ô∏è –ë–ª–æ–∫ –∏–ª–∏ –ª–∏–º–∏—Ç ({response.status_code}). –ñ–¥–µ–º 20—Å... (WARP Proxy)")
                time.sleep(20)
                continue
            
            response.raise_for_status()
            return response

        except Exception as e:
            logging.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ (WARP): {e}. –ü–æ–ø—ã—Ç–∫–∞ {i+1}/{retries}")
            time.sleep(10 * (i + 1))
    
    raise Exception(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å {url} —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ {WARP_PROXY}")

def fetch_cat_id(url, slug):
    logging.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ ID –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{slug}'...")
    r = make_request("GET", f"{url}/wp-json/wp/v2/categories?slug={slug}")
    data = r.json()
    if not data: raise RuntimeError("Cat not found")
    return data[0]["id"]

def fetch_posts(url, cid, limit):
    logging.info(f"–ó–∞–ø—Ä–æ—Å {limit} –ø–æ—Å—Ç–æ–≤ (–ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ) —á–µ—Ä–µ–∑ WARP...") 
    all_posts = []
    page = 1
    max_per_page = 100 # WordPress –Ω–µ –¥–∞–µ—Ç –±–æ–ª—å—à–µ 100 –∑–∞ —Ä–∞–∑

    while len(all_posts) < limit:
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ–ª—å–∫–æ –æ—Å—Ç–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å
            remaining = limit - len(all_posts)
            current_batch = min(remaining, max_per_page)

            logging.info(f"   üì• –°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page} ({current_batch} —à—Ç.)...")
            
            r = make_request("GET", f"{url}/wp-json/wp/v2/posts", 
                             params={
                                 "categories": cid, 
                                 "per_page": current_batch, 
                                 "page": page, 
                                 "_embed": "true"
                             })
            
            data = r.json()
            if not data:
                logging.info("   –ü–æ—Å—Ç—ã –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å —Ä–∞–Ω—å—à–µ –ª–∏–º–∏—Ç–∞.")
                break
            
            all_posts.extend(data)
            page += 1
            time.sleep(1) # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏, —á—Ç–æ–±—ã –Ω–µ –≥—Ä—É–∑–∏—Ç—å —Å–µ—Ä–≤–µ—Ä

        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")
            break
            
    return all_posts

# --- –ü–ê–†–°–ò–ù–ì (–ß–ò–°–¢–ê–Ø –í–ï–†–°–ò–Ø) ---
def parse_and_save(post, lang, stopwords):
    time.sleep(2)
    aid, slug, link = str(post["id"]), post["slug"], post.get("link")

    # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫
    try:
        raw_title = BeautifulSoup(post["title"]["rendered"], "html.parser").get_text(strip=True)
    except:
        raw_title = "No Title"
    title = sanitize_text(raw_title)

    if stopwords:
        for ph in stopwords:
            if ph in title.lower():
                logging.info(f"üö´ ID={aid}: –°—Ç–æ–ø-—Å–ª–æ–≤–æ '{ph}'")
                return None

    # 2. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ
    try:
        html_txt = make_request("GET", link).text
    except Exception: return None

    # 3. –•—ç—à (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–æ–≤)
    meta_path = OUTPUT_DIR / f"{aid}_{slug}" / "meta.json"
    curr_hash = hashlib.sha256(html_txt.encode()).hexdigest()
    if meta_path.exists():
        try:
            m = json.loads(meta_path.read_text(encoding="utf-8"))
            if m.get("hash") == curr_hash:
                logging.info(f"‚è≠Ô∏è ID={aid}: –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.")
                return m
        except: pass

    logging.info(f"Processing ID={aid}: {title[:30]}...")
    soup = BeautifulSoup(html_txt, "html.parser")

    # 4. –ü–û–î–ì–û–¢–û–í–ö–ê HTML (–£–¥–∞–ª—è–µ–º –≤—Å—ë –ª–∏—à–Ω–µ–µ –î–û –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞)
    
    # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏
    for tag in soup.find_all(["script", "style", "iframe", "noscript", "form", "button", "input", "meta", "link", "svg"]):
        tag.decompose()

    # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    c_div = soup.find("div", class_="entry-content")
    if not c_div:
        # Fallback –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ç–µ–º WP
        c_div = soup.find("div", class_="td-post-content")

    if c_div:
        # –°–ü–ò–°–û–ö –ú–£–°–û–†–ê: –∫–ª–∞—Å—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –Ω–µ –Ω—É–∂–Ω—ã
        junk_classes = [
            "post-widget-thumbnail", "related-posts", "ad-container", "share-buttons", 
            "meta-info", "jp-relatedposts", "mc_embed_signup", "widget_text", 
            "sharedaddy", "td-a-rec", "td-g-rec", "addthis_tool", "rp4wp-related-posts",
            "zeen-10-related-posts", "yarpp-related"
        ]
        
        # –£–¥–∞–ª—è–µ–º –ø–æ –∫–ª–∞—Å—Å–∞–º
        for tag in c_div.find_all(class_=True):
            classes = tag.get("class", [])
            if any(j in c for c in classes for j in junk_classes):
                tag.decompose()

        # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫–∏ "–ß–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ" –ø–æ —Ç–µ–∫—Å—Ç—É (Thaiger —á–∞—Å—Ç–æ –≤—Å—Ç–∞–≤–ª—è–µ—Ç –∏—Ö –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–º)
        for tag in c_div.find_all(["p", "h3", "h4", "div", "span"]):
            text = tag.get_text(strip=True).lower()
            if text.startswith("also read:") or text.startswith("read also:") or text.startswith("related:") or text == "advertisement":
                tag.decompose()
            # –£–¥–∞–ª—è–µ–º –∞–±–∑–∞—Ü—ã, –≥–¥–µ –æ–¥–Ω–∞ —Å—Å—ã–ª–∫–∞ (–æ–±—ã—á–Ω–æ —ç—Ç–æ –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∞)
            if tag.name == 'p' and tag.find('a') and len(tag.get_text(strip=True)) < 100:
                # –ï—Å–ª–∏ –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏ –ø–æ—á—Ç–∏ —Ä–∞–≤–Ω–∞ –¥–ª–∏–Ω–µ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ -> —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å—Å—ã–ª–∫–∞
                a_tag = tag.find('a')
                if len(a_tag.get_text(strip=True)) >= len(text) - 5:
                    tag.decompose()

    # 5. –°–ë–û–† –¢–ï–ö–°–¢–ê (–¢–æ–ª—å–∫–æ –ø–æ–ª–µ–∑–Ω—ã–µ —Ç–µ–≥–∏)
    blocks = []
    if c_div:
        # –ü—Ä–æ—Ö–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –ø–æ –∑–Ω–∞—á–∏–º—ã–º —Ç–µ–≥–∞–º –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è
        for tag in c_div.find_all(["p", "h2", "h3", "ul", "ol", "blockquote"], recursive=False):
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            if tag.name in ["h2", "h3"]:
                t = tag.get_text(strip=True)
                if t and len(t) > 3: # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–æ–≤—Å–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏-–º—É—Å–æ—Ä
                    blocks.append(f"\n<b>{t}</b>") # –ñ–∏—Ä–Ω—ã–º –≤ Telegram
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–æ–≤
            elif tag.name in ["ul", "ol"]:
                lis = [f"‚Ä¢ {li.get_text(strip=True)}" for li in tag.find_all("li") if li.get_text(strip=True)]
                if lis:
                    blocks.append("\n".join(lis))
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–∏—Ç–∞—Ç
            elif tag.name == "blockquote":
                t = tag.get_text(separator=" ", strip=True)
                if t: blocks.append(f"<i>{t}</i>")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            elif tag.name == "p":
                # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ "Photo: ..." –≤ –∫–æ–Ω—Ü–µ —Å—Ç–∞—Ç–µ–π
                t = tag.get_text(separator=" ", strip=True)
                if t.lower().startswith("photo:") or t.lower().startswith("source:"):
                    continue
                if t: blocks.append(t)

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å—ë –≤–º–µ—Å—Ç–µ —á–µ—Ä–µ–∑ –¥–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
    raw_txt_clean = "\n\n".join(blocks)
    raw_txt_clean = BAD_RE.sub("", raw_txt_clean)

    # 6. –ö–ê–†–¢–ò–ù–ö–ò
    srcs = set()
    # Lightbox (–æ–±—ã—á–Ω–æ –ª—É—á—à–∏–µ —Ñ–æ—Ç–æ)
    for link_tag in soup.find_all("a", class_="ci-lightbox", limit=10):
        if h := link_tag.get("href"): 
            if "gif" not in h.lower(): srcs.add(h)

    # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤–Ω—É—Ç—Ä–∏
    if c_div:
        for img in c_div.find_all("img"):
            if u := extract_img_url(img): srcs.add(u)

    images = []
    if srcs:
        with ThreadPoolExecutor(5) as ex:
            futs = {ex.submit(save_image, u, OUTPUT_DIR / f"{aid}_{slug}" / "images"): u for u in list(srcs)[:10]}
            for f in as_completed(futs):
                if p:=f.result(): images.append(p)

    # Fallback –Ω–∞ Featured Image
    if not images and "_embedded" in post and (m:=post["_embedded"].get("wp:featuredmedia")):
        if isinstance(m, list) and (u:=m[0].get("source_url")):
             if "300x200" not in u and "150x150" not in u and "logo" not in u.lower():
                if p:=save_image(u, OUTPUT_DIR / f"{aid}_{slug}" / "images"): images.append(p)

    # –ï—Å–ª–∏ —Å–æ–≤—Å–µ–º –Ω–µ—Ç –∫–∞—Ä—Ç–∏–Ω–æ–∫ –∏ —Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ ‚Äî —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –º—É—Å–æ—Ä, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    if not images and len(raw_txt_clean) < 100:
        logging.warning(f"‚ö†Ô∏è ID={aid}: –ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç/–Ω–µ—Ç –∫–∞—Ä—Ç–∏–Ω–æ–∫. Skip.")
        return None

    # 7. –ü–ï–†–ï–í–û–î
    final_title = title
    final_text = raw_txt_clean
    translated_lang = ""

    if lang and raw_txt_clean:
        DELIMITER = "\n|||\n" # –ù–∞–¥–µ–∂–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        text_to_translate = raw_txt_clean[:4500] 
        combined_text = f"{title}{DELIMITER}{text_to_translate}"
        
        translated_combined = translate_text(combined_text, lang)

        if translated_combined:
            if "|||" in translated_combined:
                parts = translated_combined.split("|||", 1)
                final_title = parts[0].strip()
                final_text = parts[1].strip()
            else:
                # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ - –∑–∞–≥–æ–ª–æ–≤–æ–∫, –æ—Å—Ç–∞–ª—å–Ω–æ–µ —Ç–µ–∫—Å—Ç
                parts = translated_combined.split('\n', 1)
                final_title = parts[0].strip()
                final_text = parts[1].strip() if len(parts) > 1 else ""
            translated_lang = lang
            
            if len(raw_txt_clean) > 4500:
                final_text += "\n\n..."

    final_title = sanitize_text(final_title)

    # 8. –°–û–•–†–ê–ù–ï–ù–ò–ï
    art_dir = OUTPUT_DIR / f"{aid}_{slug}"
    art_dir.mkdir(parents=True, exist_ok=True)

    (art_dir / "content.txt").write_text(raw_txt_clean, encoding="utf-8")

    meta = {
        "id": aid, "slug": slug, "date": post.get("date"), "link": link,
        "title": final_title, "text_file": "content.txt",
        "images": sorted([Path(p).name for p in images]), "posted": False,
        "hash": curr_hash, "translated_to": ""
    }

    if translated_lang:
        (art_dir / f"content.{lang}.txt").write_text(f"{final_title}\n\n{final_text}", encoding="utf-8")
        meta.update({"translated_to": lang, "text_file": f"content.{lang}.txt"})

    with open(meta_path, "w", encoding="utf-8") as f: json.dump(meta, f, ensure_ascii=False, indent=2)
    return meta

# --- MAIN ---
def main():
    # –í–æ—Ç –∑–¥–µ—Å—å –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è main.py (--base-url –∏ —Ç.–¥.)
    parser = argparse.ArgumentParser()
    parser.add_argument("--base-url", required=True)
    parser.add_argument("--slug", default="national")
    parser.add_argument("-n", "--limit", type=int, default=10)
    parser.add_argument("-l", "--lang", default="ru")
    parser.add_argument("--posted-state-file", default="articles/posted.json")
    parser.add_argument("--stopwords-file", default="stopwords.txt")
    args = parser.parse_args()

    try:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        # –û—á–∏—Å—Ç–∫—É —Å—Ç–∞—Ä—ã—Ö –ø–∞–ø–æ–∫ –º–æ–∂–Ω–æ –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, –µ—Å–ª–∏ –º–µ—à–∞–µ—Ç, –Ω–æ –æ–Ω–∞ –ø–æ–ª–µ–∑–Ω–∞
        # cleanup_old_articles(Path(args.posted_state_file), OUTPUT_DIR)

        cid = fetch_cat_id(args.base_url, args.slug)
        posts = fetch_posts(args.base_url, cid, FETCH_DEPTH)

        posted = load_posted_ids(Path(args.posted_state_file))
        stop = load_stopwords(Path(args.stopwords_file))
        catalog = []
        if CATALOG_PATH.exists():
            with open(CATALOG_PATH, 'r') as f: catalog=json.load(f)

        new_posts = [p for p in posts if str(p["id"]) not in posted]
        logging.info(f"–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}, –Ω–æ–≤—ã—Ö: {len(new_posts)}")

        if not new_posts:
            print("NEW_ARTICLES_STATUS:false")
            return

######        new_posts.reverse()
        posts_to_process = new_posts[:args.limit]
        processed = []
        count = 0

        logging.info(f"–ë—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å {len(posts_to_process)} –ø–æ—Å—Ç–æ–≤...")

        for post in posts_to_process:
            if count >= args.limit: break
            if meta := parse_and_save(post, args.lang, stop):
                processed.append(meta)
                count += 1

        if processed:
            for m in processed:
                catalog = [i for i in catalog if i.get("id") != m["id"]]
                catalog.append(m)
            try:
                catalog.sort(key=lambda x: int(x.get("id", 0)))
            except: pass
            with open(CATALOG_PATH, "w", encoding="utf-8") as f:
                json.dump(catalog, f, ensure_ascii=False, indent=2)
            print("NEW_ARTICLES_STATUS:true")
        else:
            print("NEW_ARTICLES_STATUS:false")

    except Exception:
        logging.exception("Fatal error:")
        exit(1)

if __name__ == "__main__":
    main()
